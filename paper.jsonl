{"id":"paper_2310080000000","title":"Leveraging DevOps for Scientific Computing","authors":"Paul Nuyujukian","journal":"arXiv","year":"2023","doi":"10.48550/arXiv.2310.08247","url":"https://arxiv.org/abs/2310.08247","keyAssumptions":"Scientific computing requires specialized tools; Manual environment management is feasible; Local and cluster computing environments are fundamentally different","citation":"Nuyujukian, P. (2023). Leveraging DevOps for Scientific Computing. arXiv:2310.08247.","notes":"Demonstrates that industry DevOps tools (Git+CI/CD+containers) can directly address scientific computing challenges. Key innovation: single container supporting both local interactive sessions and HPC deployment.","addedDate":"2025-08-06T06:38:00Z"}
{"id":"paper_2406010000000","title":"Formal Definition and Implementation of Reproducibility Tenets for Computational Workflows","authors":"Nicholas Pritchard et al.","journal":"arXiv","year":"2024","doi":"10.48550/arXiv.2406.01146","url":"https://arxiv.org/abs/2406.01146","keyAssumptions":"Reproducibility is informal concept; Manual verification required; Provenance tracking alone ensures reproducibility","citation":"Pritchard, N., et al. (2024). Formal Definition and Implementation of Reproducibility Tenets for Computational Workflows. arXiv:2406.01146.","notes":"Extends reproducibility concepts into formal mathematical framework with seven tenets. Uses cryptographic primitives for automated workflow signature generation and verification.","addedDate":"2025-08-06T06:38:00Z"}
{"id":"paper_2506200000000","title":"AI Copilots for Reproducibility in Science: A Case Study","authors":"Adrien Bibal et al.","journal":"arXiv","year":"2025","doi":"10.48550/arXiv.2506.20130","url":"https://arxiv.org/abs/2506.20130","keyAssumptions":"Manual reproduction is necessary; Human experts required for verification; Code and data availability ensures reproducibility","citation":"Bibal, A., et al. (2025). AI Copilots for Reproducibility in Science: A Case Study. arXiv:2506.20130.","notes":"Introduces OpenPub platform with AI-powered reproducibility copilots. Reduces reproduction time from 30+ hours to ~1 hour through automated analysis and notebook generation.","addedDate":"2025-08-06T06:38:00Z"}
{"id":"paper_2503070000000","title":"A Framework for Supporting the Reproducibility of Computational Experiments","authors":"Lázaro Costa, Susana Barbosa, Jácome Cunha","journal":"arXiv","year":"2020","doi":"10.48550/arXiv.2503.07080","url":"https://arxiv.org/html/2503.07080v2","keyAssumptions":"Domain-specific solutions needed; Manual environment recreation acceptable; Documentation completeness assumed","citation":"Costa, L., Barbosa, S., & Cunha, J. (2020). A Framework for Supporting the Reproducibility of Computational Experiments. arXiv:2503.07080.","notes":"SciRep framework for systematic experiment packaging across scientific domains. Achieves 89% success rate vs 61% for competing tools through complete environment specification.","addedDate":"2025-08-06T06:38:00Z"}
{"id":"paper_2503040000000","title":"PyPackIT: Automated Research Software Engineering for FAIR Scientific Software","authors":"Armin Ariamajd, Raquel López-Ríos de Castro, Andrea Volkamer","journal":"arXiv","year":"2025","doi":"10.48550/arXiv.2503.04921","url":"https://arxiv.org/html/2503.04921v1","keyAssumptions":"Scientists must handle software engineering manually; Research software needs custom tooling; FAIR principles require manual implementation","citation":"Ariamajd, A., López-Ríos de Castro, R., & Volkamer, A. (2025). PyPackIT: Automated Research Software Engineering for FAIR Scientific Software. arXiv:2503.04921.","notes":"Cloud-based automation tool using GitHub Actions for complete research software lifecycle. Automates software engineering best practices allowing scientists to focus on research.","addedDate":"2025-08-06T06:38:00Z"}